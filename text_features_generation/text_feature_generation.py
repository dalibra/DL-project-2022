# -*- coding: utf-8 -*-
"""text_feature_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yztCCIq9IHzVIT6LbozKb7-eJCcfEmeb

# Text feature generation
This notebook focuses on generating textual features for later use in the classifier. The features include:

- LIWC features
- BERT embeddings
- CE embeddings
- Alexa ranks

An extraction function is implemented for each feature set. To compute LIWC features and BERT embeddings, it is necessary to place the Celebrity dataset into the project folder. Additionally, the LIWC2015_English.dic file (available at https://drive.google.com/file/d/1XWJVSVGkSDLOpKQ34lRojgO_iIP_hV34/view?usp=sharing) is required for the proper functioning of the liwc library.

As the Alexa rank API is retired, a pkl-file with precomputed CE embeddings and Alexa ranks from previous research is used to incorporate them. The corresponding link to the pkl-file can be found in the relevant section of the code.
"""

# from google.colab import drive
# drive.mount('/content/drive/')
# PROJECT_FOLDER = "/content/drive/My Drive/Colab Notebooks/Deep Learning/project/" #replace it with your project folder
# !ls "$PROJECT_FOLDER"
PROJECT_FOLDER = "./"

"""To begin with, let's examine the dataset."""

import glob
import pandas as pd
import re
import pickle

#class to read Celebrity dataset
def dataset_reader(news_path, is_fake): 
    dataset = pd.DataFrame(columns=['file', 'headline', 'content', 'fake'])

    regex_pattern = r"[\n.]"
    for filename in glob.glob(news_path + "*.txt"):
        with open(filename, 'r') as file:
            text = file.read().strip()
            text_splitted = re.split(regex_pattern, text)
            text_splitted = [sentence for sentence in text_splitted if len(sentence) > 0]
            headline = ''
            for index, sentence in enumerate(text_splitted):
                headline += sentence
                if len(headline.split(' ')) >= 5:
                    break
            content = '.'.join(text_splitted[index + 1:])
            news_file = filename.split('/')[-1]
            dataset = pd.concat(
                [
                    dataset,
                    pd.DataFrame(
                        {
                            'file': news_file,
                            'headline': headline.strip(),
                            'content': content.strip(),
                            'fake': is_fake
                        }, 
                        index=[0]
                    )
                ],
                ignore_index=True
            )

    return dataset

fakes = dataset_reader(PROJECT_FOLDER + '../datasets/celebrityDataset/fake/', 1)
legits = dataset_reader(PROJECT_FOLDER + '../datasets/celebrityDataset/legit/', 0)

dataset = pd.concat([fakes, legits]).reset_index()
# dataset.sample(4)

"""## LIWC features"""

# !pip install liwc

import pandas as pd
import numpy as np
from collections import Counter
import liwc

parse, category_names = liwc.load_token_parser(PROJECT_FOLDER + 'LIWC2015_English.dic') #https://drive.google.com/file/d/1XWJVSVGkSDLOpKQ34lRojgO_iIP_hV34/view?usp=sharing
def liwc_extractor(dataset):
    features = pd.DataFrame(0, index=np.arange(dataset.shape[0]), columns = category_names)
    
    for index, row in dataset.iterrows():
        tokens = row['content'].split(' ')
        category_counts = Counter(category for token in tokens for category in parse(token))
        for category, value in category_counts.items():
            features.at[index, category] = value
    
    return features

dataset = pd.concat([liwc_extractor(dataset), dataset], axis="columns")

# dataset.sample(4)

"""## BERT embeddings"""

# !pip install transformers

from transformers import BertTokenizer, BertModel
from tqdm import tqdm

#This function compute the first 512 BERT embeddings for content of every news
def bert_extractor(dataset):
    features = pd.DataFrame(None, index=np.arange(dataset.shape[0]), columns = ["bert_features"])
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained("bert-base-uncased")
    
    for index, row in tqdm(dataset.iterrows()):
        text = row['content']
        encoded_input = tokenizer(text, return_tensors='pt')
        for key in encoded_input:
            encoded_input[key] = encoded_input[key][:, :512]
        output = model(**encoded_input)
        features.at[index, "bert_features"] = output[1][0].detach().cpu().numpy()
    
    return features

dataset = pd.concat([bert_extractor(dataset), dataset], axis="columns")
# dataset.sample(4)

"""## CE embeddings & Alexa rank"""

import gdown
import pickle

# url = 'https://drive.google.com/file/d/1BujwLdIrdGCBOjs3scSaGmv_ZhRM77lj/view' #file from previous research
# gdown.download(url,"./data.zip",quiet=False, fuzzy=True)

# !unzip -q ./data.zip

# path = '/content/multilingual_evidence_2/celebritydataset_fake_fixed.pkl'
path = PROJECT_FOLDER + 'multilingual_evidence_2/celebritydataset_fake_fixed.pkl'
file = open(path, 'rb')
data_fake = pickle.load(file)
# path = '/content/multilingual_evidence_2/celebritydataset_legit_fixed.pkl'
path = PROJECT_FOLDER + 'multilingual_evidence_2/celebritydataset_legit_fixed.pkl'
file = open(path, 'rb')
data_legit = pickle.load(file)

data_all = {**data_fake, **data_legit}

#This function just extract features' values for the first 10 scraped news from dictionary with data. If there're less than 10 news, missing values are filled with Nones. 
def sim_rank_extractor(dataset, n_news = 10):
    langs = ['en', 'fr', 'de', 'es', 'ru']
    columns = []
    for lang in langs:
        for i in range(n_news):
            columns.append(lang + "_" + str(i) + "_sim")
            columns.append(lang + "_" + str(i) + "_rank")

    features = pd.DataFrame(None, index=np.arange(dataset.shape[0]), columns=columns)
    
    for index, row in tqdm(dataset.iterrows()):
        filename = row['file']
        for lang in langs:
            for i in range(n_news):
                col_sim = lang + "_" + str(i) + "_sim"
                col_rank = lang + "_" + str(i) + "_rank"
                try:
                    features.at[index, col_sim] = data_all[filename][lang][i]["similarity"]
                except:
                    features.at[index, col_sim] = None
                try:
                    features.at[index, col_rank] = data_all[filename][lang][i]["alexa_rank"]
                except:
                    features.at[index, col_rank] = None
    
    return features

dataset = pd.concat([dataset, sim_rank_extractor(dataset)], axis="columns")
# dataset.sample(4)

"""## Save the dataset"""

dataset.to_csv(PROJECT_FOLDER + "dataset.csv")
print(dataset.sample(4))
# pd.read_csv(PROJECT_FOLDER + "dataset.csv").head() #check